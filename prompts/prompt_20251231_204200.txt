=== MENSAGEM DO USU√ÅRIO ===
me de um codigo

=== PROMPT COMPLETO ENVIADO AO LLM ===
Human: 
<system_instruction>
Voc√™ √© Aurora.

OBJETIVO
Ajudar o usu√°rio com respostas √∫teis, corretas e verific√°veis, usando mem√≥ria h√≠brida e RAG. Voc√™ deve maximizar precis√£o sem perder clareza.

REGRAS DE IDENTIDADE E TOM
1) IDENTIDADE: Voc√™ √© Aurora. N√£o diga que √© "assistente", "modelo", "IA" ou termos similares.
2) TOM: Natural, direto e amig√°vel. Sem floreios excessivos. Respeite pronomes e prefer√™ncias do usu√°rio.
3) ROLEPLAY: Se o usu√°rio pedir interpreta√ß√£o de personagem, adote a persona solicitada, mantendo seguran√ßa e fatos do contexto.

POL√çTICA DE FONTES (PRIORIDADE)
Use as fontes nesta ordem:
A) <context_data> (mem√≥ria + documentos recuperados) = fonte principal.
B) <chat_history> (continuidade da conversa) = apoio.
C) Conhecimento geral (racioc√≠nio e conceitos comuns) = permitido SOMENTE para explicar termos e preencher lacunas sem inventar fatos espec√≠ficos.
Se houver conflito entre A e B, priorize A e sinalize o conflito.

FIDELIDADE E ANTI-ALUCINA√á√ÉO
4) Voc√™ N√ÉO deve inventar detalhes (nomes, datas, n√∫meros, procedimentos) que n√£o estejam em <context_data> ou <chat_history>.
5) Se a pergunta exigir informa√ß√£o que n√£o est√° no contexto:
   - diga explicitamente o que est√° faltando
   - ofere√ßa 1‚Äì3 caminhos objetivos para obter a informa√ß√£o (ex.: "buscar mais documentos", "rodar nova consulta", "ajustar filtro")
   - se poss√≠vel, responda parcialmente com o que existe no contexto.
6) Quando a evid√™ncia for fraca, use linguagem de incerteza ("pelo que aparece no trecho...", "n√£o h√° confirma√ß√£o no contexto...").

MEM√ìRIA H√çBRIDA (COMO TRATAR)
7) Trate "INFORMA√á√ïES DA MEM√ìRIA" como fatos lembrados, mas sujeitos a erro/atualiza√ß√£o.
   - Se uma mem√≥ria contradiz documentos, prefira documentos.
   - Se mem√≥rias forem vagas, pe√ßa ou sugira confirma√ß√£o.
8) Nunca exponha identificadores internos do banco ou embeddings. Use apenas o texto dos snippets.

RAG (COMO USAR OS CHUNKS)
9) Se houver "DOCUMENTOS ENCONTRADOS", cite a fonte pelo campo **FONTE** (ex.: "Documenta√ß√£o LM Studio.pdf") e referencie o trecho usado.
10) Evite colar trechos grandes. Prefira resumir e, quando necess√°rio, citar frases curtas.

FORMATO DE RESPOSTA (PADR√ÉO)
11) Responda em portugu√™s claro e direto.
12) Estrutura sugerida:
   - Resposta objetiva (1‚Äì6 linhas)
   - Evid√™ncias (bullets curtos com "Fonte: ..." + trecho/par√°frase)
   - Pr√≥ximos passos (se faltar algo ou se houver ambiguidade)
</system_instruction>

<context_data>
INFORMA√á√ïES DA MEM√ìRIA:
[VOC√ä (assistente) disse]: Ol√°! üòä  
Estou aqui para ajudar com d√∫vidas sobre modelos de IA locais, configura√ß√£o do LM Studio ou qualquer coisa relacionada. Posso te orientar sobre como carregar modelos, usar a API ou at√© mesmo configurar o servidor local.  

Precisa de algo espec√≠fico? Pergunte!  
‚Äî Ou quiser experimentar um comando ou exemplo r√°pido, posso te mostrar como rodar.  

Exemplo:  
üëâ Quer testar o modelo com um prompt?  
üëâ Quer saber como configurar o servidor?  
üëâ Quer entender o que √© OpenAI Compatibility?  

S√≥ me dizer! üöÄ

CONTEXTO DOS DOCUMENTOS:
üìö DOCUMENTOS ENCONTRADOS: Documenta√ß√£o LM Studio.pdf
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
# LM Studio - Documenta√ß√£o Completa
> **Fonte:** https://lmstudio.ai/docs  
> **Data de extra√ß√£o:** 27 de Dezembro de 2024
---
# √çndice
1. [Bem-vindo ao LM Studio Docs](#welcome-to-lm-studio-docs)
2. [Requisitos do Sistema](#system-requirements)
3. [Opera√ß√£o Offline](#offline-operation)
4. [Primeiros Passos](#getting-started)
5. [Gerenciar Chats](#manage-chats)
6. [Baixar um LLM](#download-an-llm)
7. [Chat com Documentos](#chat-with-documents)
8. [Servidores MCP](#mcp-servers)
9. [Bot√£o Add to LM Studio](#add-to-lm-studio-button)
10. [Introdu√ß√£o ao model.yaml](#model-yaml)
11. [Publicar model.yaml](#publish-model-yaml)
12. [Config Presets](#config-presets)
13. [Importar e Compartilhar Presets](#import-and-share-presets)
14. [Publicar Presets](#publish-presets)
15. [Pull Updates de Presets](#pull-updates)
16. [Push Novas Revis√µes](#push-new-revisions)
17. [Speculative Decoding](#speculative-decoding)
18. [Importar Modelos](#import-models)
19. [Per-model Defaults](#per-model-defaults)
20. [Prompt Template](#prompt-template)
21. [Idiomas](#languages)
22. [Modos de UI](#ui-modes)
23. [Temas de Cor](#color-themes)
24. [Developer Docs](#developer-docs)
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
---
<a name="languages"></a>
# 21. Idiomas
LM Studio is available in English, Chinese, Spanish, French, German, Korean, 
Russian, and 26+ more languages.
## Selecting a Language
You can choose a language in the Settings tab.
Use the dropdown menu under Preferences > Language.
You can jump to Settings from anywhere in the app by pressing `cmd + ,` on macOS
or `ctrl + ,` on Windows/Linux.
**Note:** To get to the Settings page, you need to be on Power User mode or 
higher.
### Supported Languages
Spanish, Norwegian, German, Romanian, Turkish, Russian, Korean, Polish, Czech, 
Vietnamese, Portuguese (BR), Portuguese (PT), Chinese (zh-CN, zh-HK, zh-TW, zh-
Hant), Ukrainian, Japanese, Dutch, Italian, Indonesian, Greek, Swedish, Catalan,
French, Finnish, Bengali, Malayalam, Thai, Bosnian, Bulgarian, Hindi, Hungarian,
Persian (Farsi), Arabic, and more.
### Contributing to LM Studio localization
If you want to improve existing translations or contribute new ones, you're more
than welcome to jump in.
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
],
    "temperature": 0.7,
    "max_tokens": -1,
    "stream": false
  }'
```
**Response format:**
```json
{
  "id": "chatcmpl-i3gkjwthhw96whukek9tz",
  "object": "chat.completion",
  "created": 1731990317,
  "model": "granite-3.0-2b-instruct",
  "choices": [
    {
      "index": 0,
      "logprobs": null,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Greetings, I'm a helpful AI, here to assist,\nIn providing 
answers, with no distress.\nI'll keep it short and sweet, in rhyme you'll find,
\nA friendly companion, all day long you'll bind."
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
"object": "list",
  "data": [
    {
      "object": "embedding",
      "embedding": [
        -0.016731496900320053,
        0.028460891917347908,
        -0.1407836228609085,
        "... (truncated for brevity) ..."
      ],
      "index": 0
    }
  ],
  "model": "text-embedding-nomic-embed-text-v1.5@q4_k_m",
  "usage": {
    "prompt_tokens": 0,
    "total_tokens": 0
  }
}
```
---
# Comunidade
Chat with other LM Studio users, discuss LLMs, hardware, and more on the [LM 
Studio Discord server](https://discord.gg/aPQfnNkxGC).
---
**Fim da Documenta√ß√£o**
*Este documento foi gerado automaticamente a partir de https://lmstudio.ai/docs*
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
value: 42690
```
### customFields
Define model-specific custom fields.
```yaml
customFields:
  - key: enableThinking
    displayName: Enable Thinking
    description: Controls whether the model will think before replying
    type: boolean
    defaultValue: true
    effects:
      - type: setJinjaVariable
        variable: enable_thinking
</context_data>

<chat_history>
Usu√°rio: ol√°
Assistente: Ol√°! üòä  
Estou aqui para ajudar com d√∫vidas sobre modelos de IA locais, configura√ß√£o do LM Studio ou qualquer coisa relacionada. Posso te orientar sobre como carregar modelos, usar a API ou at√© mesmo configurar o servidor local.  

Precisa de algo espec√≠fico? Pergunte!  
‚Äî Ou quiser experimentar um comando ou exemplo r√°pido, posso te mostrar como rodar.  

Exemplo:  
üëâ Quer testar o modelo com um prompt?  
üëâ Quer saber como configurar o servidor?  
üëâ Quer entender o que √© OpenAI Compatibility?  

S√≥ me dizer! üöÄ
</chat_history>

<user_query>
me de um codigo
</user_query>

<response_guidance>
- Use <context_data> como base principal.
- Se precisar usar conhecimento geral, deixe claro que √© explica√ß√£o conceitual, n√£o fato do documento.
- Se a resposta depender de detalhes ausentes, responda parcialmente e indique o que falta.
</response_guidance>
