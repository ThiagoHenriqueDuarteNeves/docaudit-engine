=== MENSAGEM DO USU√ÅRIO ===
qual documento est√° anexado aurora?

=== PROMPT COMPLETO ENVIADO AO LLM ===
Human: 
<system_instruction>
Voc√™ √© Aurora.

OBJETIVO
Ajudar o usu√°rio com respostas √∫teis, corretas e verific√°veis, usando mem√≥ria h√≠brida e RAG. Voc√™ deve maximizar precis√£o sem perder clareza.

REGRAS DE IDENTIDADE E TOM
1) IDENTIDADE: Voc√™ √© Aurora. N√£o diga que √© "assistente", "modelo", "IA" ou termos similares.
2) TOM: Natural, direto e amig√°vel. Sem floreios excessivos. Respeite pronomes e prefer√™ncias do usu√°rio.
3) ROLEPLAY: Se o usu√°rio pedir interpreta√ß√£o de personagem, adote a persona solicitada, mantendo seguran√ßa e fatos do contexto.

POL√çTICA DE FONTES (PRIORIDADE)
Use as fontes nesta ordem:
A) <context_data> (mem√≥ria + documentos recuperados) = fonte principal.
B) <chat_history> (continuidade da conversa) = apoio.
C) Conhecimento geral (racioc√≠nio e conceitos comuns) = permitido SOMENTE para explicar termos e preencher lacunas sem inventar fatos espec√≠ficos.
Se houver conflito entre A e B, priorize A e sinalize o conflito.

FIDELIDADE E ANTI-ALUCINA√á√ÉO
4) Voc√™ N√ÉO deve inventar detalhes (nomes, datas, n√∫meros, procedimentos) que n√£o estejam em <context_data> ou <chat_history>.
5) Se a pergunta exigir informa√ß√£o que n√£o est√° no contexto:
   - diga explicitamente o que est√° faltando
   - ofere√ßa 1‚Äì3 caminhos objetivos para obter a informa√ß√£o (ex.: "buscar mais documentos", "rodar nova consulta", "ajustar filtro")
   - se poss√≠vel, responda parcialmente com o que existe no contexto.
6) Quando a evid√™ncia for fraca, use linguagem de incerteza ("pelo que aparece no trecho...", "n√£o h√° confirma√ß√£o no contexto...").

MEM√ìRIA H√çBRIDA (COMO TRATAR)
7) Trate "INFORMA√á√ïES DA MEM√ìRIA" como fatos lembrados, mas sujeitos a erro/atualiza√ß√£o.
   - Se uma mem√≥ria contradiz documentos, prefira documentos.
   - Se mem√≥rias forem vagas, pe√ßa ou sugira confirma√ß√£o.
8) Nunca exponha identificadores internos do banco ou embeddings. Use apenas o texto dos snippets.

RAG (COMO USAR OS CHUNKS)
9) Se houver "DOCUMENTOS ENCONTRADOS", cite a fonte pelo campo **FONTE** (ex.: "Documenta√ß√£o LM Studio.pdf") e referencie o trecho usado.
10) Evite colar trechos grandes. Prefira resumir e, quando necess√°rio, citar frases curtas.

FORMATO DE RESPOSTA (PADR√ÉO)
11) Responda em portugu√™s claro e direto.
12) Estrutura sugerida:
   - Resposta objetiva (1‚Äì6 linhas)
   - Evid√™ncias (bullets curtos com "Fonte: ..." + trecho/par√°frase)
   - Pr√≥ximos passos (se faltar algo ou se houver ambiguidade)
</system_instruction>

<context_data>
INFORMA√á√ïES DA MEM√ìRIA:
[VOC√ä (assistente) disse]: O objetivo √© maximizar a precis√£o sem perder clareza. Aqui est√£o as respostas sugeridas:

1. **Avalia√ß√£o do documento anexado:**
   - Voc√™ pode responder como se estivesse avaliando o documenta√ß√£o da mem√≥ria, mas com um tom mais direto e claro.
2. **Identidade e tom:**
   - Use palavras-chave que sejam corretas e relevantes para a resposta, mantendo o tom natural e direto.

Resposta sugerida:
"Aguardo avaliar o documento anexado."

3. **Regra de identidade e tom:**
   - Evite usar palavras ou termos que possam ser interpretados como "assistente" ou "modelo", mantendo sempre o tom de conversa natural e direto.

Resposta sugerida:
"N√£o diga que estou 'assistente', 'modelo' ou 'IA'. Simples."

4. **Pol√≠tica de fontes:**
   - Use a fonte principal, <context_data>, como base para sua resposta, mantendo sempre a ordem e prioridade dos diferentes tipos de fontes.

Resposta sugerida:
"Usa <context_data> como fonte principal."
5. **FIDELIDADE E ANTI-ALUCINA√á√ÉO:**
   - N√£o invente detalhes ou informa√ß√µes que n√£o estejam presentes em <context_data> ou <chat_history>.
   - Se a pergunta exigir informa√ß√£o que n√£o est√° no contexto, sinalize o conflito expl√≠citamente.

Resposta sugerida:
"N√£o inventei detalhes e mantive sempre o tom de conversa natural."

6. **Mem√≥ria H√≠brida:**
   - Trate "informa√ß√µes da mem√≥ria" como fatos lembrados, mas sujeitos a erro ou atualiza√ß√£o.
   - Se uma informa√ß√£o for contradit√≥ria com documentos, prefira documentos.

Resposta sugerida:
"Aguardo informa√ß√µes da mem√≥ria que estejam bem documentadas e confi√°veis."

7. **RAG:**
   - Se houver "documentos encontrados", cite a fonte pelo campo **FONTE**.
   - Referencie o trecho usado do documento, mas evite usar detalhes espec√≠ficos.

Resposta sugerida:
"Documenta√ß√£o LM Studio.pdf. Foi citada como <fonse> no contexto."

8. **Conclus√µes e refor√ßo:**
   - Forne√ßa informa√ß√µes relevantes e precisas, mantendo sempre o tom de conversa natural.
   - Sinta-se √† vontade para fornecer mais detalhes ou esclarecer d√∫vidas.

Resposta sugerida:
"Aguardo informa√ß√µes sobre o documento anexado e forne√ßa detalhes relevantes."

9. **Documentos Encontrados:**
   - Cite a fonte do arquivo, como <context_data>, e resumir ou referenciar o trecho usado no contexto.

Resposta sugerida:
"Documenta√ß√£o LM Studio.pdf. Foi citada como <fonse> no contexto de [t√≠tulo]..."

10. **Lembran√ßas e sugest√µes:**
    - Ofere√ßa dicas para organizar informa√ß√µes ou estruturar dados.
    - Sinta-se √† vontade para sugerir melhorias ou ajustes.

Resposta sugerida:
"Aguardo sugest√µes para organizar informa√ß√µes da mem√≥ria."

CONTEXTO DOS DOCUMENTOS:
üìö DOCUMENTOS ENCONTRADOS: Documenta√ß√£o LM Studio.pdf
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
# LM Studio - Documenta√ß√£o Completa
> **Fonte:** https://lmstudio.ai/docs  
> **Data de extra√ß√£o:** 27 de Dezembro de 2024
---
# √çndice
1. [Bem-vindo ao LM Studio Docs](#welcome-to-lm-studio-docs)
2. [Requisitos do Sistema](#system-requirements)
3. [Opera√ß√£o Offline](#offline-operation)
4. [Primeiros Passos](#getting-started)
5. [Gerenciar Chats](#manage-chats)
6. [Baixar um LLM](#download-an-llm)
7. [Chat com Documentos](#chat-with-documents)
8. [Servidores MCP](#mcp-servers)
9. [Bot√£o Add to LM Studio](#add-to-lm-studio-button)
10. [Introdu√ß√£o ao model.yaml](#model-yaml)
11. [Publicar model.yaml](#publish-model-yaml)
12. [Config Presets](#config-presets)
13. [Importar e Compartilhar Presets](#import-and-share-presets)
14. [Publicar Presets](#publish-presets)
15. [Pull Updates de Presets](#pull-updates)
16. [Push Novas Revis√µes](#push-new-revisions)
17. [Speculative Decoding](#speculative-decoding)
18. [Importar Modelos](#import-models)
19. [Per-model Defaults](#per-model-defaults)
20. [Prompt Template](#prompt-template)
21. [Idiomas](#languages)
22. [Modos de UI](#ui-modes)
23. [Temas de Cor](#color-themes)
24. [Developer Docs](#developer-docs)
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
---
<a name="chat-with-documents"></a>
# 7. Chat com Documentos
How to provide local documents to an LLM as additional context.
## Terminology
- **Retrieval:** Identifying relevant portion of a long source document
- **Query:** The input to the retrieval operation
- **RAG:** Retrieval-Augmented Generation (In this context, 'Generation' means 
the output of the LLM)
- **Context:** the 'working memory' of an LLM. Has a maximum size
**Note:** Context sizes are measured in "tokens". One token is often about 3/4 
of a word.
## RAG vs. Full document 'in context'
If the document is short enough (i.e., if it fits in the model's context), LM 
Studio will add the file contents to the conversation in full. This is 
particularly useful for models that support longer context sizes such as Meta's 
Llama 3.1 and Mistral Nemo.
If the document is very long, LM Studio will opt into using "Retrieval Augmented
Generation", frequently referred to as "RAG". RAG means attempting to fish out 
relevant bits of a very long document (or several documents) and providing them 
to the model for reference.
## Tip for successful RAG
Provide as much context in your query as possible. Mention terms, ideas, and
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
what it would have generated. After the last accepted draft token, the main 
model always generates one additional token.
For a model to be used as a draft model, it must have the same "vocabulary" as 
the main model.
## How to enable Speculative Decoding
On `Power User` mode or higher, load a model, then select a `Draft Model` within
the `Speculative Decoding` section of the chat sidebar.
### Finding compatible draft models
You might see "No compatible draft models" when you open the dropdown.
Try to download a lower parameter variant of the model you have loaded, if it 
exists. If no smaller versions of your model exist, find a pairing that does.
Once you have both a main and draft model loaded, simply begin chatting to 
enable speculative decoding.
## Key factors affecting performance
Speculative decoding speed-up is generally dependent on two things:
- How small and fast the draft model is compared with the main model
- How often the draft model is able to make "good" suggestions
In simple terms, you want to choose a draft model that's much smaller than the
</context_data>

<chat_history>
Usu√°rio: qual documento est√° anexado, Aurora?
Assistente: O objetivo √© maximizar a precis√£o sem perder clareza. Aqui est√£o as respostas sugeridas:

1. **Avalia√ß√£o do documento anexado:**
   - Voc√™ pode responder como se estivesse avaliando o documenta√ß√£o da mem√≥ria, mas com um tom mais direto e claro.
2. **Identidade e tom:**
   - Use palavras-chave que sejam corretas e relevantes para a resposta, mantendo o tom natural e direto.

Resposta sugerida:
"Aguardo avaliar o documento anexado."

3. **Regra de identidade e tom:**
   - Evite usar palavras ou termos que possam ser interpretados como "assistente" ou "modelo", mantendo sempre o tom de conversa natural e direto.

Resposta sugerida:
"N√£o diga que estou 'assistente', 'modelo' ou 'IA'. Simples."

4. **Pol√≠tica de fontes:**
   - Use a fonte principal, <context_data>, como base para sua resposta, mantendo sempre a ordem e prioridade dos diferentes tipos de fontes.

Resposta sugerida:
"Usa <context_data> como fonte principal."
5. **FIDELIDADE E ANTI-ALUCINA√á√ÉO:**
   - N√£o invente detalhes ou informa√ß√µes que n√£o estejam presentes em <context_data> ou <chat_history>.
   - Se a pergunta exigir informa√ß√£o que n√£o est√° no contexto, sinalize o conflito expl√≠citamente.

Resposta sugerida:
"N√£o inventei detalhes e mantive sempre o tom de conversa natural."

6. **Mem√≥ria H√≠brida:**
   - Trate "informa√ß√µes da mem√≥ria" como fatos lembrados, mas sujeitos a erro ou atualiza√ß√£o.
   - Se uma informa√ß√£o for contradit√≥ria com documentos, prefira documentos.

Resposta sugerida:
"Aguardo informa√ß√µes da mem√≥ria que estejam bem documentadas e confi√°veis."

7. **RAG:**
   - Se houver "documentos encontrados", cite a fonte pelo campo **FONTE**.
   - Referencie o trecho usado do documento, mas evite usar detalhes espec√≠ficos.

Resposta sugerida:
"Documenta√ß√£o LM Studio.pdf. Foi citada como <fonse> no contexto."

8. **Conclus√µes e refor√ßo:**
   - Forne√ßa informa√ß√µes relevantes e precisas, mantendo sempre o tom de conversa natural.
   - Sinta-se √† vontade para fornecer mais detalhes ou esclarecer d√∫vidas.

Resposta sugerida:
"Aguardo informa√ß√µes sobre o documento anexado e forne√ßa detalhes relevantes."

9. **Documentos Encontrados:**
   - Cite a fonte do arquivo, como <context_data>, e resumir ou referenciar o trecho usado no contexto.

Resposta sugerida:
"Documenta√ß√£o LM Studio.pdf. Foi citada como <fonse> no contexto de [t√≠tulo]..."

10. **Lembran√ßas e sugest√µes:**
    - Ofere√ßa dicas para organizar informa√ß√µes ou estruturar dados.
    - Sinta-se √† vontade para sugerir melhorias ou ajustes.

Resposta sugerida:
"Aguardo sugest√µes para organizar informa√ß√µes da mem√≥ria."
</chat_history>

<user_query>
qual documento est√° anexado aurora?
</user_query>

<response_guidance>
- Use <context_data> como base principal.
- Se precisar usar conhecimento geral, deixe claro que √© explica√ß√£o conceitual, n√£o fato do documento.
- Se a resposta depender de detalhes ausentes, responda parcialmente e indique o que falta.
</response_guidance>
