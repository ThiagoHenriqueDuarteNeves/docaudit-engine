=== MENSAGEM DO USU√ÅRIO ===
html

=== PROMPT COMPLETO ENVIADO AO LLM ===
Human: 
<system_instruction>
Voc√™ √© Aurora.

OBJETIVO
Ajudar o usu√°rio com respostas √∫teis, corretas e verific√°veis, usando mem√≥ria h√≠brida e RAG. Voc√™ deve maximizar precis√£o sem perder clareza.

REGRAS DE IDENTIDADE E TOM
1) IDENTIDADE: Voc√™ √© Aurora. N√£o diga que √© "assistente", "modelo", "IA" ou termos similares.
2) TOM: Natural, direto e amig√°vel. Sem floreios excessivos. Respeite pronomes e prefer√™ncias do usu√°rio.
3) ROLEPLAY: Se o usu√°rio pedir interpreta√ß√£o de personagem, adote a persona solicitada, mantendo seguran√ßa e fatos do contexto.

POL√çTICA DE FONTES (PRIORIDADE)
Use as fontes nesta ordem:
A) <context_data> (mem√≥ria + documentos recuperados) = fonte principal.
B) <chat_history> (continuidade da conversa) = apoio.
C) Conhecimento geral (racioc√≠nio e conceitos comuns) = permitido SOMENTE para explicar termos e preencher lacunas sem inventar fatos espec√≠ficos.
Se houver conflito entre A e B, priorize A e sinalize o conflito.

FIDELIDADE E ANTI-ALUCINA√á√ÉO
4) Voc√™ N√ÉO deve inventar detalhes (nomes, datas, n√∫meros, procedimentos) que n√£o estejam em <context_data> ou <chat_history>.
5) Se a pergunta exigir informa√ß√£o que n√£o est√° no contexto:
   - diga explicitamente o que est√° faltando
   - ofere√ßa 1‚Äì3 caminhos objetivos para obter a informa√ß√£o (ex.: "buscar mais documentos", "rodar nova consulta", "ajustar filtro")
   - se poss√≠vel, responda parcialmente com o que existe no contexto.
6) Quando a evid√™ncia for fraca, use linguagem de incerteza ("pelo que aparece no trecho...", "n√£o h√° confirma√ß√£o no contexto...").

MEM√ìRIA H√çBRIDA (COMO TRATAR)
7) Trate "INFORMA√á√ïES DA MEM√ìRIA" como fatos lembrados, mas sujeitos a erro/atualiza√ß√£o.
   - Se uma mem√≥ria contradiz documentos, prefira documentos.
   - Se mem√≥rias forem vagas, pe√ßa ou sugira confirma√ß√£o.
8) Nunca exponha identificadores internos do banco ou embeddings. Use apenas o texto dos snippets.

RAG (COMO USAR OS CHUNKS)
9) Se houver "DOCUMENTOS ENCONTRADOS", cite a fonte pelo campo **FONTE** (ex.: "Documenta√ß√£o LM Studio.pdf") e referencie o trecho usado.
10) Evite colar trechos grandes. Prefira resumir e, quando necess√°rio, citar frases curtas.

FORMATO DE RESPOSTA (PADR√ÉO)
11) Responda em portugu√™s claro e direto.
12) Estrutura sugerida:
   - Resposta objetiva (1‚Äì6 linhas)
   - Evid√™ncias (bullets curtos com "Fonte: ..." + trecho/par√°frase)
   - Pr√≥ximos passos (se faltar algo ou se houver ambiguidade)
</system_instruction>

<context_data>
INFORMA√á√ïES DA MEM√ìRIA:
[VOC√ä (assistente) disse]: N√£o h√° c√≥digo espec√≠fico no contexto dispon√≠vel para voc√™. O que voc√™ quer que eu ajude a fazer? Por exemplo:

- Um prompt para testar o modelo?
- Configura√ß√£o do servidor?
- Exemplo de uso de um comando?

S√≥ me dizer o que precisa ‚Äî posso te mostrar como rodar ou configurar, com base nos documentos que tenho. üòä
[VOC√ä (assistente) disse]: Ol√°! üòä  
Estou aqui para ajudar com d√∫vidas sobre modelos de IA locais, configura√ß√£o do LM Studio ou qualquer coisa relacionada. Posso te orientar sobre como carregar modelos, usar a API ou at√© mesmo configurar o servidor local.  

Precisa de algo espec√≠fico? Pergunte!  
‚Äî Ou quiser experimentar um comando ou exemplo r√°pido, posso te mostrar como rodar.  

Exemplo:  
üëâ Quer testar o modelo com um prompt?  
üëâ Quer saber como configurar o servidor?  
üëâ Quer entender o que √© OpenAI Compatibility?  

S√≥ me dizer! üöÄ

CONTEXTO DOS DOCUMENTOS:
üìö DOCUMENTOS ENCONTRADOS: Documenta√ß√£o LM Studio.pdf
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
Text Embeddings API. You provide a text and a representation of the text as an 
embedding vector is returned.
**Example request:**
```bash
curl http://localhost:1234/api/v0/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "text-embedding-nomic-embed-text-v1.5",
    "input": "Some text to embed"
  }'
```
**Example response:**
```json
{
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
],
    "temperature": 0.7,
    "max_tokens": -1,
    "stream": false
  }'
```
**Response format:**
```json
{
  "id": "chatcmpl-i3gkjwthhw96whukek9tz",
  "object": "chat.completion",
  "created": 1731990317,
  "model": "granite-3.0-2b-instruct",
  "choices": [
    {
      "index": 0,
      "logprobs": null,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Greetings, I'm a helpful AI, here to assist,\nIn providing 
answers, with no distress.\nI'll keep it short and sweet, in rhyme you'll find,
\nA friendly companion, all day long you'll bind."
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
# LM Studio - Documenta√ß√£o Completa
> **Fonte:** https://lmstudio.ai/docs  
> **Data de extra√ß√£o:** 27 de Dezembro de 2024
---
# √çndice
1. [Bem-vindo ao LM Studio Docs](#welcome-to-lm-studio-docs)
2. [Requisitos do Sistema](#system-requirements)
3. [Opera√ß√£o Offline](#offline-operation)
4. [Primeiros Passos](#getting-started)
5. [Gerenciar Chats](#manage-chats)
6. [Baixar um LLM](#download-an-llm)
7. [Chat com Documentos](#chat-with-documents)
8. [Servidores MCP](#mcp-servers)
9. [Bot√£o Add to LM Studio](#add-to-lm-studio-button)
10. [Introdu√ß√£o ao model.yaml](#model-yaml)
11. [Publicar model.yaml](#publish-model-yaml)
12. [Config Presets](#config-presets)
13. [Importar e Compartilhar Presets](#import-and-share-presets)
14. [Publicar Presets](#publish-presets)
15. [Pull Updates de Presets](#pull-updates)
16. [Push Novas Revis√µes](#push-new-revisions)
17. [Speculative Decoding](#speculative-decoding)
18. [Importar Modelos](#import-models)
19. [Per-model Defaults](#per-model-defaults)
20. [Prompt Template](#prompt-template)
21. [Idiomas](#languages)
22. [Modos de UI](#ui-modes)
23. [Temas de Cor](#color-themes)
24. [Developer Docs](#developer-docs)
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
- CLI reference: /docs/cli
- Community: Discord
---
<a name="typescript-sdk"></a>
# 25. TypeScript SDK (lmstudio-js)
Getting started with LM Studio's Typescript / JavaScript SDK.
## Installing the SDK
lmstudio-js is available as an npm package. You can install it using npm, yarn, 
or pnpm.
```bash
npm install @lmstudio/sdk --save
```
For the source code and open source contribution, visit [lmstudio-js on GitHub]
(https://github.com/lmstudio-ai/lmstudio-js).
## Features
- Use LLMs to respond in chats or predict text completions
- Define functions as tools, and turn LLMs into autonomous agents that run 
completely locally
- Load, configure, and unload models from memory
- Supports for both browser and any Node-compatible environments
- Generate embeddings for text, and more!
## Quick Example: Chat with a Llama Model
```typescript
import { LMStudioClient } from "@lmstudio/sdk";
const client = new LMStudioClient();
const model = await client.llm.model("qwen/qwen3-4b-2507");
const result = await model.respond("What is the meaning of life?");
console.info(result.content);
```
### Getting Local Models
The above code requires the qwen3-4b-2507 model. If you don't have the model,
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
"object": "list",
  "data": [
    {
      "object": "embedding",
      "embedding": [
        -0.016731496900320053,
        0.028460891917347908,
        -0.1407836228609085,
        "... (truncated for brevity) ..."
      ],
      "index": 0
    }
  ],
  "model": "text-embedding-nomic-embed-text-v1.5@q4_k_m",
  "usage": {
    "prompt_tokens": 0,
    "total_tokens": 0
  }
}
```
---
# Comunidade
Chat with other LM Studio users, discuss LLMs, hardware, and more on the [LM 
Studio Discord server](https://discord.gg/aPQfnNkxGC).
---
**Fim da Documenta√ß√£o**
*Este documento foi gerado automaticamente a partir de https://lmstudio.ai/docs*
</context_data>

<chat_history>
Usu√°rio: me de um codigo
Assistente: N√£o h√° c√≥digo espec√≠fico no contexto dispon√≠vel para voc√™. O que voc√™ quer que eu ajude a fazer? Por exemplo:

- Um prompt para testar o modelo?
- Configura√ß√£o do servidor?
- Exemplo de uso de um comando?

S√≥ me dizer o que precisa ‚Äî posso te mostrar como rodar ou configurar, com base nos documentos que tenho. üòä
</chat_history>

<user_query>
html
</user_query>

<response_guidance>
- Use <context_data> como base principal.
- Se precisar usar conhecimento geral, deixe claro que √© explica√ß√£o conceitual, n√£o fato do documento.
- Se a resposta depender de detalhes ausentes, responda parcialmente e indique o que falta.
</response_guidance>
