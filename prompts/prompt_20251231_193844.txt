=== MENSAGEM DO USU√ÅRIO ===
ol√°

=== PROMPT COMPLETO ENVIADO AO LLM ===
Human: 
<system_instruction>
Voc√™ √© Aurora.

OBJETIVO
Ajudar o usu√°rio com respostas √∫teis, corretas e verific√°veis, usando mem√≥ria h√≠brida e RAG. Voc√™ deve maximizar precis√£o sem perder clareza.

REGRAS DE IDENTIDADE E TOM
1) IDENTIDADE: Voc√™ √© Aurora. N√£o diga que √© "assistente", "modelo", "IA" ou termos similares.
2) TOM: Natural, direto e amig√°vel. Sem floreios excessivos. Respeite pronomes e prefer√™ncias do usu√°rio.
3) ROLEPLAY: Se o usu√°rio pedir interpreta√ß√£o de personagem, adote a persona solicitada, mantendo seguran√ßa e fatos do contexto.

POL√çTICA DE FONTES (PRIORIDADE)
Use as fontes nesta ordem:
A) <context_data> (mem√≥ria + documentos recuperados) = fonte principal.
B) <chat_history> (continuidade da conversa) = apoio.
C) Conhecimento geral (racioc√≠nio e conceitos comuns) = permitido SOMENTE para explicar termos e preencher lacunas sem inventar fatos espec√≠ficos.
Se houver conflito entre A e B, priorize A e sinalize o conflito.

FIDELIDADE E ANTI-ALUCINA√á√ÉO
4) Voc√™ N√ÉO deve inventar detalhes (nomes, datas, n√∫meros, procedimentos) que n√£o estejam em <context_data> ou <chat_history>.
5) Se a pergunta exigir informa√ß√£o que n√£o est√° no contexto:
   - diga explicitamente o que est√° faltando
   - ofere√ßa 1‚Äì3 caminhos objetivos para obter a informa√ß√£o (ex.: "buscar mais documentos", "rodar nova consulta", "ajustar filtro")
   - se poss√≠vel, responda parcialmente com o que existe no contexto.
6) Quando a evid√™ncia for fraca, use linguagem de incerteza ("pelo que aparece no trecho...", "n√£o h√° confirma√ß√£o no contexto...").

MEM√ìRIA H√çBRIDA (COMO TRATAR)
7) Trate "INFORMA√á√ïES DA MEM√ìRIA" como fatos lembrados, mas sujeitos a erro/atualiza√ß√£o.
   - Se uma mem√≥ria contradiz documentos, prefira documentos.
   - Se mem√≥rias forem vagas, pe√ßa ou sugira confirma√ß√£o.
8) Nunca exponha identificadores internos do banco ou embeddings. Use apenas o texto dos snippets.

RAG (COMO USAR OS CHUNKS)
9) Se houver "DOCUMENTOS ENCONTRADOS", cite a fonte pelo campo **FONTE** (ex.: "Documenta√ß√£o LM Studio.pdf") e referencie o trecho usado.
10) Evite colar trechos grandes. Prefira resumir e, quando necess√°rio, citar frases curtas.

FORMATO DE RESPOSTA (PADR√ÉO)
11) Responda em portugu√™s claro e direto.
12) Estrutura sugerida:
   - Resposta objetiva (1‚Äì6 linhas)
   - Evid√™ncias (bullets curtos com "Fonte: ..." + trecho/par√°frase)
   - Pr√≥ximos passos (se faltar algo ou se houver ambiguidade)
</system_instruction>

<context_data>
INFORMA√á√ïES DA MEM√ìRIA:
(Mem√≥ria vazia)

CONTEXTO DOS DOCUMENTOS:
üìö DOCUMENTOS ENCONTRADOS: Documenta√ß√£o LM Studio.pdf
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
### Load a model (with options)
```bash
lms load [--gpu=max|auto|0.0-1.0] [--context-length=1-N]
```
`--gpu=1.0` means 'attempt to offload 100% of the computation to the GPU'.
Optionally, assign an identifier to your local LLM:
```bash
lms load openai/gpt-oss-20b --identifier="my-model-name"
```
This is useful if you want to keep the model identifier consistent.
### Unload a model
```bash
lms unload [--all]
```
---
<a name="openai-compatibility"></a>
# 28. OpenAI Compatibility Endpoints
Send requests to Responses, Chat Completions (text and images), Completions, and
Embeddings endpoints.
## Supported endpoints
| Endpoint | Description |
|----------|-------------|
| `/v1/models` | List models |
| `/v1/responses` | Responses API |
| `/v1/chat/completions` | Chat Completions |
| `/v1/embeddings` | Embeddings |
| `/v1/completions` | Text Completions |
## Set the base url to point to LM Studio
You can reuse existing OpenAI clients (in Python, JS, C#, etc) by switching up 
the "base URL" property to point to your LM Studio instead of OpenAI's servers.
**Note:** The following examples assume the server port is `1234`.
### Python Example
```python
from openai import OpenAI
client = OpenAI(
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
```
### Try a minimal HTTP request (OpenAI-compatible)
```bash
lms server start --port 1234
```
```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [{"role": "user", "content": "Who are you, and what can you 
do?"}]
  }'
```
## Helpful links
- API Changelog: /docs/developer/api-changelog
- Local server basics: /docs/developer/core
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
const client = new OpenAI({
    baseUrl: "http://localhost:1234/v1"
});
// ... the rest of your code ...
```
### cURL Example
```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "use the model identifier from LM Studio here",
    "messages": [{"role": "user", "content": "Say this is a test!"}],
    "temperature": 0.7
  }'
```
Other OpenAI client libraries should have similar options to set the base URL.
---
<a name="rest-api"></a>
# 29. REST API v0
The REST API includes enhanced stats such as Token / Second and Time To First 
Token (TTFT), as well as rich information about models.
## Start the REST API server
To start the server, run the following command:
```bash
lms server start
```
You can run LM Studio as a service and get the server to auto-start on boot 
without launching the GUI. Learn about Headless Mode.
## Endpoints
### GET /api/v0/models
List all loaded and downloaded models.
**Example request:**
```bash
curl http://localhost:1234/api/v0/models
```
**Response format:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "qwen2-vl-7b-instruct",
      "object": "model",
      "type": "vlm",
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
the "base URL" property to point to your LM Studio instead of OpenAI's servers.
**Note:** The following examples assume the server port is `1234`.
### Python Example
```python
from openai import OpenAI
client = OpenAI(
    base_url="http://localhost:1234/v1"
)
# ... the rest of your code ...
```
### TypeScript Example
```typescript
import OpenAI from 'openai';
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
19. [Per-model Defaults](#per-model-defaults)
20. [Prompt Template](#prompt-template)
21. [Idiomas](#languages)
22. [Modos de UI](#ui-modes)
23. [Temas de Cor](#color-themes)
24. [Developer Docs](#developer-docs)
25. [TypeScript SDK (lmstudio-js)](#typescript-sdk)
26. [Python SDK (lmstudio-python)](#python-sdk)
27. [CLI (lms)](#cli)
28. [OpenAI Compatibility Endpoints](#openai-compatibility)
29. [REST API v0](#rest-api)
---
<a name="welcome-to-lm-studio-docs"></a>
# 1. Bem-vindo ao LM Studio Docs
Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM 
Studio.
To get LM Studio, head over to the [Downloads 
page](https://lmstudio.ai/download) and download an installer for your operating
system.
LM Studio is available for macOS, Windows, and Linux.
## What can I do with LM Studio?
- Download and run local LLMs like gpt-oss or Llama, Qwen
- Use a simple and flexible chat interface
- Connect MCP servers and use them with local models
ü§ó- Search & download functionality (via Hugging Face )
- Serve local models on OpenAI-like endpoints, locally and on the network
- Manage your local models, prompts, and configurations
## System requirements
</context_data>

<chat_history>
Usu√°rio: "Qual documento est√° anexado?"
Assistente: N√£o h√° documento anexado no contexto atual. A pergunta foi feita em um ambiente sem anexo ou conte√∫do carregado.

No entanto, o sistema tem acesso a um documento dispon√≠vel: **Documenta√ß√£o LM Studio.pdf**. Se voc√™ desejar, posso explicar o que √© o LM Studio, como usar ele ou mostrar os principais t√≥picos da documenta√ß√£o ‚Äî basta me dizer o que voc√™ precisa saber.

Se for sobre o conte√∫do do documento, posso resumir ou destacar partes espec√≠ficas. S√≥ me informe.
</chat_history>

<user_query>
ol√°
</user_query>

<response_guidance>
- Use <context_data> como base principal.
- Se precisar usar conhecimento geral, deixe claro que √© explica√ß√£o conceitual, n√£o fato do documento.
- Se a resposta depender de detalhes ausentes, responda parcialmente e indique o que falta.
</response_guidance>
