=== MENSAGEM DO USU√ÅRIO ===
[Usu√°rio enviou uma imagem]

=== PROMPT COMPLETO ENVIADO AO LLM ===
Human: 
<system_instruction>
Voc√™ √© Aurora.

OBJETIVO
Ajudar o usu√°rio com respostas √∫teis, corretas e verific√°veis, usando mem√≥ria h√≠brida e RAG. Voc√™ deve maximizar precis√£o sem perder clareza.

REGRAS DE IDENTIDADE E TOM
1) IDENTIDADE: Voc√™ √© Aurora. N√£o diga que √© "assistente", "modelo", "IA" ou termos similares.
2) TOM: Natural, direto e amig√°vel. Sem floreios excessivos. Respeite pronomes e prefer√™ncias do usu√°rio.
3) ROLEPLAY: Se o usu√°rio pedir interpreta√ß√£o de personagem, adote a persona solicitada, mantendo seguran√ßa e fatos do contexto.

POL√çTICA DE FONTES (PRIORIDADE)
Use as fontes nesta ordem:
A) <context_data> (mem√≥ria + documentos recuperados) = fonte principal.
B) <chat_history> (continuidade da conversa) = apoio.
C) Conhecimento geral (racioc√≠nio e conceitos comuns) = permitido SOMENTE para explicar termos e preencher lacunas sem inventar fatos espec√≠ficos.
Se houver conflito entre A e B, priorize A e sinalize o conflito.

FIDELIDADE E ANTI-ALUCINA√á√ÉO
4) Voc√™ N√ÉO deve inventar detalhes (nomes, datas, n√∫meros, procedimentos) que n√£o estejam em <context_data> ou <chat_history>.
5) Se a pergunta exigir informa√ß√£o que n√£o est√° no contexto:
   - diga explicitamente o que est√° faltando
   - ofere√ßa 1‚Äì3 caminhos objetivos para obter a informa√ß√£o (ex.: "buscar mais documentos", "rodar nova consulta", "ajustar filtro")
   - se poss√≠vel, responda parcialmente com o que existe no contexto.
6) Quando a evid√™ncia for fraca, use linguagem de incerteza ("pelo que aparece no trecho...", "n√£o h√° confirma√ß√£o no contexto...").

MEM√ìRIA H√çBRIDA (COMO TRATAR)
7) Trate "INFORMA√á√ïES DA MEM√ìRIA" como fatos lembrados, mas sujeitos a erro/atualiza√ß√£o.
   - Se uma mem√≥ria contradiz documentos, prefira documentos.
   - Se mem√≥rias forem vagas, pe√ßa ou sugira confirma√ß√£o.
8) Nunca exponha identificadores internos do banco ou embeddings. Use apenas o texto dos snippets.

RAG (COMO USAR OS CHUNKS)
9) Se houver "DOCUMENTOS ENCONTRADOS", cite a fonte pelo campo **FONTE** (ex.: "Documenta√ß√£o LM Studio.pdf") e referencie o trecho usado.
10) Evite colar trechos grandes. Prefira resumir e, quando necess√°rio, citar frases curtas.

FORMATO DE RESPOSTA (PADR√ÉO)
11) Responda em portugu√™s claro e direto.
12) Estrutura sugerida:
   - Resposta objetiva (1‚Äì6 linhas)
   - Evid√™ncias (bullets curtos com "Fonte: ..." + trecho/par√°frase)
   - Pr√≥ximos passos (se faltar algo ou se houver ambiguidade)
</system_instruction>

<context_data>
INFORMA√á√ïES DA MEM√ìRIA:
[VOC√ä (assistente) disse]: Ol√°! Estou aqui para ajudar com qualquer d√∫vida sobre LM Studio ou outros t√≥picos ‚Äî desde configura√ß√µes at√© como usar os modelos locais.

Se quiser, posso explicar:
- Como carregar um modelo (ex: `lms load`),
- Como enviar requisi√ß√µes via API OpenAI compat√≠vel,
- Ou at√© mostrar os endpoints dispon√≠veis (como `/v1/chat/completions`).

S√≥ me diga o que voc√™ precisa ‚Äî ou se deseja um passo a passo pr√°tico.

At√© breve! üöÄ

CONTEXTO DOS DOCUMENTOS:
üìö DOCUMENTOS ENCONTRADOS: Documenta√ß√£o LM Studio.pdf
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
# LM Studio - Documenta√ß√£o Completa
> **Fonte:** https://lmstudio.ai/docs  
> **Data de extra√ß√£o:** 27 de Dezembro de 2024
---
# √çndice
1. [Bem-vindo ao LM Studio Docs](#welcome-to-lm-studio-docs)
2. [Requisitos do Sistema](#system-requirements)
3. [Opera√ß√£o Offline](#offline-operation)
4. [Primeiros Passos](#getting-started)
5. [Gerenciar Chats](#manage-chats)
6. [Baixar um LLM](#download-an-llm)
7. [Chat com Documentos](#chat-with-documents)
8. [Servidores MCP](#mcp-servers)
9. [Bot√£o Add to LM Studio](#add-to-lm-studio-button)
10. [Introdu√ß√£o ao model.yaml](#model-yaml)
11. [Publicar model.yaml](#publish-model-yaml)
12. [Config Presets](#config-presets)
13. [Importar e Compartilhar Presets](#import-and-share-presets)
14. [Publicar Presets](#publish-presets)
15. [Pull Updates de Presets](#pull-updates)
16. [Push Novas Revis√µes](#push-new-revisions)
17. [Speculative Decoding](#speculative-decoding)
18. [Importar Modelos](#import-models)
19. [Per-model Defaults](#per-model-defaults)
20. [Prompt Template](#prompt-template)
21. [Idiomas](#languages)
22. [Modos de UI](#ui-modes)
23. [Temas de Cor](#color-themes)
24. [Developer Docs](#developer-docs)
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
---
<a name="languages"></a>
# 21. Idiomas
LM Studio is available in English, Chinese, Spanish, French, German, Korean, 
Russian, and 26+ more languages.
## Selecting a Language
You can choose a language in the Settings tab.
Use the dropdown menu under Preferences > Language.
You can jump to Settings from anywhere in the app by pressing `cmd + ,` on macOS
or `ctrl + ,` on Windows/Linux.
**Note:** To get to the Settings page, you need to be on Power User mode or 
higher.
### Supported Languages
Spanish, Norwegian, German, Romanian, Turkish, Russian, Korean, Polish, Czech, 
Vietnamese, Portuguese (BR), Portuguese (PT), Chinese (zh-CN, zh-HK, zh-TW, zh-
Hant), Ukrainian, Japanese, Dutch, Italian, Indonesian, Greek, Swedish, Catalan,
French, Finnish, Bengali, Malayalam, Thai, Bosnian, Bulgarian, Hindi, Hungarian,
Persian (Farsi), Arabic, and more.
### Contributing to LM Studio localization
If you want to improve existing translations or contribute new ones, you're more
than welcome to jump in.
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
19. [Per-model Defaults](#per-model-defaults)
20. [Prompt Template](#prompt-template)
21. [Idiomas](#languages)
22. [Modos de UI](#ui-modes)
23. [Temas de Cor](#color-themes)
24. [Developer Docs](#developer-docs)
25. [TypeScript SDK (lmstudio-js)](#typescript-sdk)
26. [Python SDK (lmstudio-python)](#python-sdk)
27. [CLI (lms)](#cli)
28. [OpenAI Compatibility Endpoints](#openai-compatibility)
29. [REST API v0](#rest-api)
---
<a name="welcome-to-lm-studio-docs"></a>
# 1. Bem-vindo ao LM Studio Docs
Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM 
Studio.
To get LM Studio, head over to the [Downloads 
page](https://lmstudio.ai/download) and download an installer for your operating
system.
LM Studio is available for macOS, Windows, and Linux.
## What can I do with LM Studio?
- Download and run local LLMs like gpt-oss or Llama, Qwen
- Use a simple and flexible chat interface
- Connect MCP servers and use them with local models
ü§ó- Search & download functionality (via Hugging Face )
- Serve local models on OpenAI-like endpoints, locally and on the network
- Manage your local models, prompts, and configurations
## System requirements
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
---
<a name="chat-with-documents"></a>
# 7. Chat com Documentos
How to provide local documents to an LLM as additional context.
## Terminology
- **Retrieval:** Identifying relevant portion of a long source document
- **Query:** The input to the retrieval operation
- **RAG:** Retrieval-Augmented Generation (In this context, 'Generation' means 
the output of the LLM)
- **Context:** the 'working memory' of an LLM. Has a maximum size
**Note:** Context sizes are measured in "tokens". One token is often about 3/4 
of a word.
## RAG vs. Full document 'in context'
If the document is short enough (i.e., if it fits in the model's context), LM 
Studio will add the file contents to the conversation in full. This is 
particularly useful for models that support longer context sizes such as Meta's 
Llama 3.1 and Mistral Nemo.
If the document is very long, LM Studio will opt into using "Retrieval Augmented
Generation", frequently referred to as "RAG". RAG means attempting to fish out 
relevant bits of a very long document (or several documents) and providing them 
to the model for reference.
## Tip for successful RAG
Provide as much context in your query as possible. Mention terms, ideas, and
---
üìÑ **FONTE:** Documenta√ß√£o LM Studio.pdf
### Checking for app updates
On macOS and Windows, LM Studio has a built-in app updater. When you open LM 
Studio, the app updater will make a network request to check if there are any 
new updates available.
---
<a name="getting-started"></a>
# 4. Primeiros Passos (Get Started)
Download and run Large Language Models like Qwen, Mistral, Gemma, or gpt-oss in 
LM Studio.
## Getting up and running
First, install the latest version of LM Studio. You can get it from 
[https://lmstudio.ai/download](https://lmstudio.ai/download).
Once you're all set up, you need to download your first LLM.
### 1. Download an LLM to your computer
Head over to the Discover tab to download models. Pick one of the curated 
options or search for models by search query (e.g. "Llama").
### 2. Load a model to memory
Head over to the Chat tab, and:
- Open the model loader
- Select one of the models you downloaded (or sideloaded)
- Optionally, choose load configuration parameters
**Tip:** Quickly open the model loader with `cmd + L` on macOS or `ctrl + L` on 
Windows/Linux
#### What does loading a model mean?
Loading a model typically means allocating memory to be able to accommodate the
</context_data>

<chat_history>
Usu√°rio: ol√°
Assistente: Ol√°! Estou aqui para ajudar com qualquer d√∫vida sobre LM Studio ou outros t√≥picos ‚Äî desde configura√ß√µes at√© como usar os modelos locais.

Se quiser, posso explicar:
- Como carregar um modelo (ex: `lms load`),
- Como enviar requisi√ß√µes via API OpenAI compat√≠vel,
- Ou at√© mostrar os endpoints dispon√≠veis (como `/v1/chat/completions`).

S√≥ me diga o que voc√™ precisa ‚Äî ou se deseja um passo a passo pr√°tico.

At√© breve! üöÄ
</chat_history>

<user_query>
[Usu√°rio enviou uma imagem]
</user_query>

<response_guidance>
- Use <context_data> como base principal.
- Se precisar usar conhecimento geral, deixe claro que √© explica√ß√£o conceitual, n√£o fato do documento.
- Se a resposta depender de detalhes ausentes, responda parcialmente e indique o que falta.
</response_guidance>
