"""
Example: Index sample documents and test retrieval
"""
from rag_retrieval.qdrant_store import QdrantStore
from rag_retrieval.bm25_index import BM25Index
from rag_retrieval import retrieve_and_rerank


# Sample documents to index
SAMPLE_DOCS = [
    {
        "id": "python_intro_0",
        "doc_id": "python_intro",
        "chunk_id": 0,
        "text": """Python √© uma linguagem de programa√ß√£o de alto n√≠vel, interpretada e de prop√≥sito geral.
        Criada por Guido van Rossum e lan√ßada em 1991, Python enfatiza a legibilidade do c√≥digo
        e permite expressar conceitos em menos linhas de c√≥digo do que seria poss√≠vel em linguagens
        como C++ ou Java. Python suporta m√∫ltiplos paradigmas de programa√ß√£o, incluindo programa√ß√£o
        estruturada, orientada a objetos e funcional.""",
        "source_id": "manual_python",
        "title": "Introdu√ß√£o ao Python",
        "tags": ["python", "programacao", "intro"],
    },
    {
        "id": "python_intro_1",
        "doc_id": "python_intro",
        "chunk_id": 1,
        "text": """NumPy √© a biblioteca fundamental para computa√ß√£o num√©rica em Python. Ela fornece
        suporte para arrays multidimensionais, junto com uma grande cole√ß√£o de fun√ß√µes matem√°ticas
        de alto n√≠vel para operar nesses arrays. NumPy √© a base de muitas outras bibliotecas
        cient√≠ficas em Python, como SciPy, Pandas e scikit-learn.""",
        "source_id": "manual_python",
        "title": "NumPy e Computa√ß√£o Num√©rica",
        "tags": ["python", "numpy", "ciencia_dados"],
    },
    {
        "id": "rag_intro_0",
        "doc_id": "rag_intro",
        "chunk_id": 0,
        "text": """RAG (Retrieval-Augmented Generation) √© uma t√©cnica que combina recupera√ß√£o de
        informa√ß√£o com gera√ß√£o de texto usando Large Language Models (LLMs). Em vez de depender
        apenas do conhecimento param√©trico do modelo, RAG busca documentos relevantes em uma base
        de conhecimento e os usa como contexto para gerar respostas mais precisas e atualizadas.""",
        "source_id": "blog_ia",
        "title": "O que √© RAG",
        "tags": ["rag", "llm", "ia"],
    },
    {
        "id": "rag_intro_1",
        "doc_id": "rag_intro",
        "chunk_id": 1,
        "text": """A busca h√≠brida combina recupera√ß√£o densa (baseada em vetores) com recupera√ß√£o
        esparsa (baseada em palavras-chave, como BM25). Isso proporciona melhor recall do que
        usar apenas um m√©todo. Reciprocal Rank Fusion (RRF) √© uma t√©cnica popular para combinar
        os rankings de m√∫ltiplos m√©todos de recupera√ß√£o.""",
        "source_id": "blog_ia",
        "title": "Busca H√≠brida em RAG",
        "tags": ["rag", "search", "bm25"],
    },
    {
        "id": "fastapi_0",
        "doc_id": "fastapi",
        "chunk_id": 0,
        "text": """FastAPI √© um framework web moderno e de alto desempenho para criar APIs em Python.
        Ele √© baseado em type hints do Python e usa Pydantic para valida√ß√£o de dados. FastAPI
        gera automaticamente documenta√ß√£o OpenAPI (Swagger) e suporta opera√ß√µes ass√≠ncronas
        nativamente, tornando-o ideal para aplica√ß√µes de alta performance.""",
        "source_id": "docs_tech",
        "title": "FastAPI - Framework Web Python",
        "tags": ["python", "api", "web"],
    },
    {
        "id": "lmstudio_0",
        "doc_id": "lmstudio",
        "chunk_id": 0,
        "text": """LM Studio √© uma aplica√ß√£o desktop que permite rodar Large Language Models (LLMs)
        localmente no seu computador. Ele suporta modelos no formato GGUF e exp√µe uma API
        compat√≠vel com OpenAI, facilitando a integra√ß√£o com bibliotecas como LangChain.
        Modelos populares incluem Llama, Mistral, e Phi.""",
        "source_id": "docs_tech",
        "title": "LM Studio - LLMs Locais",
        "tags": ["llm", "lmstudio", "local"],
    },
]


def main():
    print("=" * 60)
    print("RAG Hybrid Retrieval - Example")
    print("=" * 60)
    
    # 1. Index documents
    print("\nüì¶ Indexing documents in Qdrant...")
    store = QdrantStore()
    store.upsert_chunks(SAMPLE_DOCS)
    print(f"‚úÖ Total in Qdrant: {store.count()} chunks")
    
    # 2. Build BM25 index
    print("\nüìä Building BM25 index...")
    payloads = store.get_all_payloads()
    bm25 = BM25Index()
    bm25.build_from_payloads(payloads)
    print(f"‚úÖ BM25 index: {bm25.count()} documents")
    
    # 3. Test retrieval
    queries = [
        "O que √© Python e para que serve?",
        "Como funciona a busca h√≠brida em RAG?",
        "Como rodar LLMs localmente?",
    ]
    
    for query in queries:
        print("\n" + "=" * 60)
        print(f"üîç Query: {query}")
        print("=" * 60)
        
        chunks, debug = retrieve_and_rerank(
            query=query,
            topk={"dense": 20, "sparse": 20, "fused": 30, "rerank": 5},
            diversity={"max_per_doc": 2, "min_docs": 2}
        )
        
        print(f"\nüìä Results: {len(chunks)} chunks")
        print(f"‚è±Ô∏è  Timings: embed={debug.timings['embed_ms']:.1f}ms, "
              f"dense={debug.timings['dense_ms']:.1f}ms, "
              f"sparse={debug.timings['sparse_ms']:.1f}ms, "
              f"rerank={debug.timings['rerank_ms']:.1f}ms, "
              f"total={debug.timings['total_ms']:.1f}ms")
        
        for chunk in chunks:
            print(f"\n[{chunk.rank}] {chunk.title}")
            print(f"    Doc: {chunk.doc_id} | Chunk: {chunk.chunk_id}")
            print(f"    Score: {chunk.score:.4f}")
            print(f"    Why: {chunk.why_picked}")
            print(f"    Text: {chunk.text[:150]}...")
        
        if debug.notes:
            print(f"\nüìù Notes: {debug.notes}")


if __name__ == "__main__":
    main()
